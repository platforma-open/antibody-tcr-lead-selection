// Prerun template for clonotype filtering
wf := import("@platforma-sdk/workflow-tengo:workflow")
exec := import("@platforma-sdk/workflow-tengo:exec")
assets := import("@platforma-sdk/workflow-tengo:assets")
xsv := import("@platforma-sdk/workflow-tengo:pframes.xsv")
pframes := import("@platforma-sdk/workflow-tengo:pframes")
slices := import("@platforma-sdk/workflow-tengo:slices")
render := import("@platforma-sdk/workflow-tengo:render")
ll := import("@platforma-sdk/workflow-tengo:ll")
pt := import("@platforma-sdk/workflow-tengo:pt")
text := import("text")
json := import("json")

dataUtils := import(":libs.data-utils")
spectratypeConv := import(":libs.pf-spectratype-conv")
vjUsageConv := import(":libs.pf-vj-usage-conv")
sampledColsConv := import(":libs.sampled-cols-conv")
kabatConv := import(":libs.pf-kabat-conv")


wf.prepare(func(args){
	if is_undefined(args.inputAnchor) {
        return {
            columns: wf.createPBundleBuilder().build()
        }
    }
	// We need a table with cluster ID (optional) | clonotype id | selected ranking columns
    bundleBuilder := wf.createPBundleBuilder()
    bundleBuilder.ignoreMissingDomains() // to make query work for both bulk and single cell data
    bundleBuilder.addAnchor("main", args.inputAnchor) 
    
    validRanks := false
    if len(args.rankingOrder) > 0 {
        for col in args.rankingOrder {
            // For cases where the user is selecting the table to filter
            if col.value != undefined {
                bundleBuilder.addAnchor(col.value.anchorName, col.value.anchorRef)
                bundleBuilder.addSingle(col.value.column)
                validRanks = true
            }
        }
    } 
    if !validRanks {
        // @TODO: this is a temporal patch for issue where rankingOrderDefault 
        // are not defined by the time prerun works
        // prerun sometimes runs before this variable is ready
        if args.rankingOrderDefault.value != undefined {
            bundleBuilder.addAnchor(args.rankingOrderDefault.value.anchorName, 
                                    args.rankingOrderDefault.value.anchorRef)
            bundleBuilder.addSingle(args.rankingOrderDefault.value.column)
        }
    }

    // Load filter columns
    if len(args.filters) > 0 {
        for filter in args.filters {
            if filter.value != undefined {
                bundleBuilder.addAnchor(filter.value.anchorName, filter.value.anchorRef)
                bundleBuilder.addSingle(filter.value.column)
            }
        }
    }
    

    // Add linker column
    bundleBuilder.addMulti({
        axes: [{ anchor: "main", idx: 1 }], // this will do partial axes match (unlike in the model)
        annotations: { "pl7.app/isLinkerColumn": "true" },
        partialAxesMatch: true
    }, "linkers")

    // Add cluster size columns from clustering blocks
    bundleBuilder.addMulti({
        name: "pl7.app/vdj/clustering/clusterSize",
        partialAxesMatch: true
    }, "clusterSizes")

    // Add CDR3 sequences
	bundleBuilder.addMulti({
		axes: [{ anchor: "main", idx: 1 }], // Clonotype axis
		name: "pl7.app/vdj/sequence",
		domain: {
			"pl7.app/alphabet": "aminoacid",
			"pl7.app/vdj/feature": "CDR3"    // Specify CDR3 feature
		}
	}, "cdr3Sequences") // New collection name for CDR3 sequences

	// Add V gene
	bundleBuilder.addMulti({
		axes: [{ anchor: "main", idx: 1 }], // Clonotype axis
		name: "pl7.app/vdj/geneHit",
		domain: {
			"pl7.app/vdj/reference": "VGene"
		}
	}, "VGenes")

	// Add J gene
	bundleBuilder.addMulti({
		axes: [{ anchor: "main", idx: 1 }], // Clonotype axis
		name: "pl7.app/vdj/geneHit",
		domain: {
			"pl7.app/vdj/reference": "JGene"
		}
	}, "JGenes")

	// Add assembling feature aminoacid sequences (bulk, sc, scFv)
	bundleBuilder.addMulti({
		axes: [{ anchor: "main", idx: 1 }], // Clonotype axis
		annotations: { "pl7.app/vdj/isAssemblingFeature": "true" },
		domain: { "pl7.app/alphabet": "aminoacid" }
	}, "assemblingAaSeqs")
    
    return {
        columns: bundleBuilder.build()
    }
})

wf.body(func(args) {
	// output containers 
	outputs := {}

    if !is_undefined(args.inputAnchor) {
        // Input arguments
        columns := args.columns
        datasetSpec := columns.getSpec(args.inputAnchor)
        topClonotypes := args.topClonotypes

        // Needed conditional variable
	    isSingleCell := datasetSpec.axesSpec[1].name == "pl7.app/vdj/scClonotypeKey"

        ////////// Clonotype Filtering //////////
        clonotypeData := dataUtils.prepareClonotypeData(args.filters, args.rankingOrder, args.rankingOrderDefault, columns, datasetSpec)
        structuredMap := clonotypeData.structuredMap
        axisRenames := clonotypeData.axisRenames
        filterMap := clonotypeData.filterMap
        rankingMap := clonotypeData.rankingMap
        addedCols := clonotypeData.addedCols
        linkerAxisSpec := clonotypeData.linkerAxisSpec

        // Continue only if we have at least a column
        // This condition prevents temporal intermittent error while filters are 
        // being processed and possibly in other situations too
        if addedCols {
            // Run ptabler-based filtering (matches filter.py logic)
            filterResult := dataUtils.filterClonotypes(structuredMap, axisRenames, filterMap, datasetSpec)
            // Run sampling script if topClonotypes is defined
            finalClonotypesParquet := undefined
            if args.topClonotypes != undefined {
                sampleClones := exec.builder().
                    software(assets.importSoftware("@platforma-open/milaboratories.top-antibodies.sample-clonotypes:main")).
                    mem("16GiB").
                    cpu(1).
                    addFile("filteredClonotypes.parquet", filterResult.filteredParquet).
                    arg("--input").arg("filteredClonotypes.parquet").
                    arg("--n").arg(string(topClonotypes)).
                    arg("--ranking-map").arg(string(json.encode(rankingMap))).
                    arg("--out").arg("sampledClonotypes_top.csv").
                    arg("--out-parquet").arg("sampledClonotypes_top.parquet").
                    saveFile("sampledClonotypes_top.csv").
                    saveFile("sampledClonotypes_top.parquet").
                    printErrStreamToStdout().
                    saveStdoutContent().
                    cache(24 * 60 * 60 * 1000).
                    run()
                
                finalClonotypesCsv := sampleClones.getFile("sampledClonotypes_top.csv")
                sampledColumnsPf := xsv.importFile(finalClonotypesCsv, "csv",
                    sampledColsConv.getColumns(datasetSpec, true), {cpu: 1, mem: "4GiB"})
                outputs["sampledRows"] = pframes.exportFrame(sampledColumnsPf)
                finalClonotypesParquet = sampleClones.getFile("sampledClonotypes_top.parquet")
            } else {
                // No sampling, use filtered parquet as final output
                finalClonotypesParquet = filterResult.filteredParquet
                outputs["sampledRows"] = pframes.exportFrame(filterResult.pframe)
            }
            ////////// CDR3 Length Calculation //////////
            cdr3Data := dataUtils.prepareCdr3Data(columns, datasetSpec, isSingleCell)
            cdr3SeqStructuredMap := cdr3Data.structuredMap
            cdr3SeqAxisRenames := cdr3Data.axisRenames

            // Build ptabler workflow
            wfCdr3Seq := pt.workflow().cacheInputs(24 * 60 * 60 * 1000)
            cdr3SeqProjection := []
            for origAxis, aliasName in cdr3SeqAxisRenames {
                cdr3SeqProjection = append(cdr3SeqProjection, pt.axis(origAxis).alias(aliasName))
            }
            for colName, _ in cdr3SeqStructuredMap {
                cdr3SeqProjection = append(cdr3SeqProjection, pt.col(colName))
            }

            dfCdr3Seq := wfCdr3Seq.frame(pt.p.full(cdr3SeqStructuredMap)).select(cdr3SeqProjection...)
            dfCdr3Seq.save("cdr3_sequences.parquet")
            cdr3SeqResult := wfCdr3Seq.run()
            cdr3SeqParquet := cdr3SeqResult.getFile("cdr3_sequences.parquet")

            cdr3VspectratypeCmd := exec.builder().
                software(assets.importSoftware("@platforma-open/milaboratories.top-antibodies.spectratype:main")).
                mem("16GiB").
                cpu(1).
                addFile("cdr3_sequences_input.parquet", cdr3SeqParquet).
                arg("--input").arg("cdr3_sequences_input.parquet").
                arg("--spectratype_tsv").arg("spectratype.tsv").
                arg("--vj_usage_tsv").arg("vj_usage.tsv") // no dot here

            // Add top clonotypes argument and file to the builder if provided
            if finalClonotypesParquet != undefined {
                cdr3VspectratypeCmd = cdr3VspectratypeCmd.
                    arg("--final_clonotypes_parquet").arg("finalClonotypes.parquet").
                    addFile("finalClonotypes.parquet", finalClonotypesParquet)
            }

            cdr3VspectratypeCmd = cdr3VspectratypeCmd. // continue building the command
                saveFile("spectratype.tsv").
                saveFile("vj_usage.tsv").
                printErrStreamToStdout().
                cache(24 * 60 * 60 * 1000).
                run()

            // Spectratype PFrame structure is [chain][cdr3Length][vGene] -> count
            cdr3VspectratypePf := xsv.importFile(cdr3VspectratypeCmd.getFile("spectratype.tsv"), 
                                                "tsv", spectratypeConv.getColumns(),
                                                {cpu: 1, mem: "4GiB"})
            outputs["cdr3VspectratypePf"] = pframes.exportFrame(cdr3VspectratypePf) 

            // For vjUsage structure is [chain][vGene][jGene] -> count
            vjUsagePf := xsv.importFile(cdr3VspectratypeCmd.getFile("vj_usage.tsv"), 
                                        "tsv", vjUsageConv.getColumns(), 
                                        {cpu: 1, mem: "4GiB"})
            outputs["vjUsagePf"] = pframes.exportFrame(vjUsagePf)

            if args.kabatNumbering == true {
            ////////// Assembling AA sequences //////////
                assemSeqTable := pframes.tsvFileBuilder()
                keyHeader := "clonotypeKey"
                assemSeqTable.setAxisHeader(datasetSpec.axesSpec[1].name, keyHeader)

                seqCols := columns.getColumns("assemblingAaSeqs")
                for col in seqCols {
                    headerName := dataUtils.makeHeaderName(col, "assemblingFeature", isSingleCell)
                    assemSeqTable.add(col, {header: headerName})
                }

                assemSeqTable.mem("16GiB")
                assemSeqTable.cpu(1)
                assemSeqTableBuilt := assemSeqTable.build()

                // Convert assembling feature sequences to FASTA via sub-template
                assemFastaTpl := assets.importTemplate(":assembling-fasta")
                bulkChain := undefined
                if !isSingleCell {
                    // infer bulk chain by header names of incoming seq columns (domain uses IGHeavy / IGLight)
                    chainDetected := "KL"
                    for col in seqCols {
                        ch := col.spec.axesSpec[0].domain["pl7.app/vdj/chain"] // e.g., IGHeavy, IGLight
                        if ch == "IGHeavy" { chainDetected = "H"; break }
                        if ch == "IGLight" { chainDetected = "KL" }
                    }
                    bulkChain = chainDetected
                }
                assem := render.create(assemFastaTpl, {
                    inputTsv: assemSeqTableBuilt,
                    keyColumn: "clonotypeKey",
                    finalClonotypesParquet: finalClonotypesParquet,
                    isSingleCell: isSingleCell,
                    bulkChain: bulkChain
                })
                //outputs["assemblingAnarci"] = assem.output("anarci", 24 * 60 * 60 * 1000)
                kabatFile := assem.output("kabat", 24 * 60 * 60 * 1000)
                // Derive feature name from assembling feature columns (prefer first column's feature)
                featName := ""
                if len(seqCols) > 0 {
                    f := seqCols[0].spec.domain["pl7.app/vdj/feature"]
                    if f != undefined { featName = f }
                }
                // Convert kabat.tsv to PFrame with proper specs (bulk: select heavy/light)
                kabatPf := xsv.importFile(kabatFile, "tsv", kabatConv.getColumns(datasetSpec, featName, bulkChain), {cpu: 1, mem: "8GiB"})
                outputs["assemblingKabatPf"] = pframes.exportFrame(kabatPf)
            }
        }
    }

	return {
		outputs: outputs,
		exports: {}
	}
}) 